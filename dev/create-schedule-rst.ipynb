{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xlrd import XLRDError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://github.com/harmsm/physical-biochemistry/blob/master/\"\n",
    "col_to_keep = [\"Week\",\"Date\",\"Topic\",\"Due\",\"Reading\",\"Material\"]\n",
    "schedule_file = \"schedule.xlsx\"\n",
    "\n",
    "stick_on_top = \\\n",
    "\"\"\"\n",
    "Schedule\n",
    "========\n",
    "\n",
    "+ Reading should be done *before* the class it covers.  The abbreviation \"MoL\"\n",
    "  indicates the \"Molecules of Life\" text.  The abbreviation \"SSTB\" indicates the\n",
    "  \"Simple Statistical Thermodynamics for the Biochemist\" packet. \n",
    "+ The class materials (notes, handouts, etc.) will be posted after the class\n",
    "  session in the \"Material\" column.\n",
    "+ Lab and homework material will be updated throughout the course.\n",
    "+ This schedule is approximate and subject to revision, with the exception of\n",
    "  the dates of the exams.\n",
    "  \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _update_link_dict(new_key,new_value,current_dict):\n",
    "    \n",
    "    try:\n",
    "        current_dict[new_key]\n",
    "        err = \"link key {} is not unique.\\n\".format(items[j])\n",
    "        raise ValueError(err)\n",
    "    except:\n",
    "        current_dict[new_key] = new_value\n",
    "\n",
    "    return current_dict\n",
    "\n",
    "\n",
    "df = pd.read_excel(schedule_file,sheet_name=\"schedule\")\n",
    "df = df.filter(items=col_to_keep,axis=1)\n",
    "\n",
    "# Read dictionary of links.  If none specified, will be empty dict\n",
    "try:\n",
    "    link_df = pd.read_excel(schedule_file,sheet_name=\"links\")\n",
    "    link_dict = dict(zip(link_df[\"link_name\"],link_df[\"link_url\"]))\n",
    "except XLRDError:\n",
    "    link_dict = {}\n",
    "\n",
    "# Fix NaN -> \"\"\n",
    "for c in df:\n",
    "    for i, r in enumerate(df[c]):\n",
    "        try:\n",
    "            if np.isnan(r):\n",
    "                df.loc[i,c] = \"\"\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "# Split and add links\n",
    "hw_links = {}\n",
    "for i, r in enumerate(df[\"Due\"]):\n",
    "    items = [item.strip() for item in r.split(\",\")]\n",
    "    for j in range(len(items)):\n",
    "        if items[j].startswith(\"HW\"):\n",
    "            try:\n",
    "                link_target = link_dict[items[j]]\n",
    "            except KeyError:\n",
    "                link_target = \"MISSING_LINK\"\n",
    "            items[j] = \"{}_\".format(items[j])\n",
    "            hw_links = _update_link_dict(items[j],link_target,hw_links)\n",
    "    if len(items) == 1 and items[0] == \"\":\n",
    "        items = [\"---\"]\n",
    "\n",
    "    df.loc[i,\"Due\"] = \", \".join(items)\n",
    "\n",
    "reading_links = {}\n",
    "for i, r in enumerate(df[\"Reading\"]):\n",
    "    items = [item.strip() for item in r.split(\",\")]\n",
    "    \n",
    "    if len(items) == 1 and items[0] == \"\":\n",
    "        items = [\"---\"]\n",
    "    \n",
    "    else:\n",
    "        for j in range(len(items)):\n",
    "            if not items[j].startswith(\"MoL\"):\n",
    "\n",
    "                if items[j].startswith(\"SSTB\"):\n",
    "                    link_target = \"readings/sstb.pdf\"\n",
    "\n",
    "                elif items[j] == \"\":\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        link_target = link_dict[items[j]]\n",
    "                    except KeyError:\n",
    "                        link_target = \"MISSING_LINK\"\n",
    "\n",
    "                # Clean up and make look like a link\n",
    "                items[j] = \"`{}`_\".format(items[j])\n",
    "\n",
    "                reading_links = _update_link_dict(items[j],link_target,reading_links)\n",
    "            \n",
    "    df.loc[i,\"Reading\"] = \", \".join(items)\n",
    "    \n",
    "material_links = {}\n",
    "for i, r in enumerate(df[\"Material\"]):\n",
    "    items = [item.strip() for item in r.split(\",\")]\n",
    "    \n",
    "    if len(items) == 1 and items[0] == \"\":\n",
    "        items = [\"---\"]\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        for j in range(len(items)):\n",
    "\n",
    "            try:\n",
    "                link_target = link_dict[items[j]]\n",
    "            except KeyError:\n",
    "                link_target = \"MISSING_LINK\"\n",
    "\n",
    "            # Clean up and make look like a link\n",
    "            items[j] = \"`{}`_\".format(items[j])\n",
    "\n",
    "            material_links = _update_link_dict(items[j],link_target,material_links)\n",
    "            \n",
    "    df.loc[i,\"Material\"] = \", \".join(items)\n",
    "    \n",
    "    \n",
    "lab_links = {}\n",
    "for i, r in enumerate(df[\"Topic\"]):\n",
    "    if r.startswith(\"Lab\"):\n",
    "\n",
    "        link_alias = \"`{}`_\".format(r.strip())\n",
    "        \n",
    "        try:\n",
    "            link_target = link_dict[r.strip()]\n",
    "        except KeyError:\n",
    "            link_target = \"MISSING_LINK\"\n",
    "        \n",
    "        lab_links = _update_link_dict(link_alias,link_target,lab_links)\n",
    "        \n",
    "        df.loc[i,\"Topic\"] = link_alias\n",
    "        \n",
    "        \n",
    "# Find length of colums\n",
    "col_length = {}\n",
    "for c in df:\n",
    "            \n",
    "    tmp = [len(c)]\n",
    "    for i, r in enumerate(df[c]):\n",
    "\n",
    "        tmp.append(len(\"{}\".format(r).strip()))\n",
    "\n",
    "    col_length[c] = max(tmp)        \n",
    "\n",
    "    \n",
    "##### BUILD RST ##########\n",
    "    \n",
    "header_strings = []\n",
    "row_strings = []\n",
    "fmt_strings = []\n",
    "for c in df:\n",
    "    header_strings.append(\"+=\" + (col_length[c] + 1)*\"=\")\n",
    "    row_strings.append(\"+-\" + (col_length[c] + 1)*\"-\")\n",
    "    fmt_strings.append(\"| {{:{:}}} \".format(col_length[c]))\n",
    "    \n",
    "header_strings.append(\"+\")\n",
    "row_strings.append(\"+\")\n",
    "    \n",
    "row = \"-\".join(row_strings)\n",
    "header = \"=\".join(header_strings)\n",
    "\n",
    "out = []\n",
    "out.append(row)\n",
    "\n",
    "tmp_out = []\n",
    "for i, c in enumerate(df):\n",
    "    tmp_out.append(fmt_strings[i].format(c))    \n",
    "tmp_out.append(\"|\")\n",
    "out.append(\" \".join(tmp_out))\n",
    "out.append(header)\n",
    "\n",
    "for r in df.iterrows():\n",
    "    \n",
    "    tmp_out = []\n",
    "    for i, c in enumerate(df):\n",
    "        tmp_out.append(fmt_strings[i].format(r[1][c]))\n",
    "    tmp_out.append(\"|\")\n",
    "    out.append(\" \".join(tmp_out))\n",
    "    out.append(row)\n",
    "    \n",
    "out.append(\"\")\n",
    "if base_url[-1] == \"/\":\n",
    "    base_url = base_url[:-1]\n",
    "\n",
    "out.append(\".. reading links\")\n",
    "for k in reading_links:\n",
    "    alias = k[:-1]\n",
    "    link = reading_links[k]\n",
    "    if link[0:4] not in [\"http\",\"ftp:\"]:\n",
    "        link = \"{}/{}\".format(base_url,link)\n",
    "    out.append(\".. _{}: {}\".format(alias,link))\n",
    "out.append(\"\")\n",
    "\n",
    "out.append(\".. material links\")\n",
    "for k in material_links:\n",
    "    alias = k[:-1]\n",
    "    link = material_links[k]\n",
    "    if link[0:4] not in [\"http\",\"ftp:\"]:\n",
    "        link = \"{}/{}\".format(base_url,link)\n",
    "    out.append(\".. _{}: {}\".format(alias,link))\n",
    "out.append(\"\")\n",
    "\n",
    "out.append(\".. lab links\")\n",
    "for k in lab_links:\n",
    "    alias = k[:-1]\n",
    "    link = lab_links[k]\n",
    "    if link[0:4] not in [\"http\",\"ftp:\"]:\n",
    "        link = \"{}/{}\".format(base_url,link)\n",
    "    out.append(\".. _{}: {}\".format(alias,link))\n",
    "out.append(\"\")\n",
    "\n",
    "out.append(\".. homework links\")\n",
    "for k in hw_links:\n",
    "    alias = k[:-1]\n",
    "    link = hw_links[k]\n",
    "    if link[0:4] not in [\"http\",\"ftp:\"]:\n",
    "        link = \"{}/{}\".format(base_url,link)\n",
    "    out.append(\".. _{}: {}\".format(alias,link))\n",
    "out.append(\"\")\n",
    "\n",
    " \n",
    "f = open(\"junk.rst\",\"w\")\n",
    "f.write(stick_on_top)\n",
    "f.write(\"\\n\".join(out))\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
